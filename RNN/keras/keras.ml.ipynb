{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, Embedding, LSTM\n",
    "from tensorflow.keras import Input\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from tensorflow.strings import regex_replace\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "b'Witty. Quirky. Genuine. Surreal. Butterfly wings? One could ask what all of these words best describe, and some (those in fuse with the international film community) may quickly say Happenstance, but others may jump aboard the more American train and immediately yell, The Butterfly Effect. Strangely, I would be one of those screaming for that sci-fi Kutcher film mainly because none of those words that I initially mentioned at the start of this paragraph accurately depicts the Tautou feature that I witnessed. Sure, we all loved her in Amelie and thought she was the daughter of Jesus in The Da Vinci Code, but in this film first-time director (of a feature film at least) Laurent Firode doesn\\'t give Tautou the opportunity to shine. Sadly, he gives nobody the opportunity to really demonstrate themselves because he is too delicately caught up in the moments of \"random chance\" to bring this film to anything but just a shimmer (never a true boil). Firode has ample, and I use \"ample\" as a small word, moments throughout this film where he could have built us a fantastical story, a genuinely whimsical fairy-tale of love and coincidence, but instead he fell face-first into a mud-bucket of chaotic intertwining that overwhelmed us with inconsistent characters and a story that left us gasping for less.  Tautou\\'s beautiful face adorns the cover of this box, but do not be so taken immediately as I did in assuming that this was going to be another monumental journey into Tautou\\'s French cinema. Tautou is in this film, do not get me wrong, but one could argue that she is not at the center of this story. Firode\\'s job is to create a series of random events that eventually will lead to an audience friendly (albeit confusing) ending which exemplifies that meaning of refreshing \"melodrama\". He utterly, utterly fails. Firode fails by giving us, the audience, too many characters. With too many characters he gives us too many random interventions, and by the end you don\\'t really care who is who, or what is what, or how is how; your main focus happens to be centered solely on the ending credits and the time destination of their arrival. Tautou could have saved this film from the disaster it was if only Firode would have given her the center. Alas, he did not, but attempted to seemingly force a group of 12 through a theoretical film hole about the size of a penny. It just didn\\'t work and we were left with a jam in which we were completely stuck.  Firode fails because he focus\\' so intently on the minor details that, for one of those rare film occurrences, he actually forgets the central focus. I can say that there was no defined central focus to Happenstance. In the beginning he attempts to create one with our two supposed main characters discovering that they share the same birthday and their horoscope promises love by the moonlight, but we never go back to that throughout the film. Instead, again, we are bombarded with new characters, stuffy scenes, and meaningless drivel obviously chosen to direct us away from an actual story and more into a world full of \"ifs, ands, and buts\". I couldn\\'t do it. I couldn\\'t believe this film. Writer Firode (yes, the same guy directing this garbage) uses a technique so primitive in this film that I immediately felt like ending it immediately. He must have been assuming that many of us were incapable of actually following the storyline (or the scientific premise) because he grabs the aid of a homeless person to actually fill in the respective blanks. I didn\\'t need this, nor do I think Firode needed to belittle his audience in this matter. While there were other elements that just didn\\'t seem to work for me at all (again, felt like a jumbled Parisian collage of shredded paper), this was the icing on the cake. I don\\'t need my hand held through films.  I will give this film one star for credit. This is a rather difficult genre to master successfully. Time travel films are especially hard because of the innumerable amounts of possibilities that are never accounted for, but with Happenstance it works because Firode semi-explores the different avenues. While I will counter with saying that he does not do it well, it did make for at least five full minutes of enjoyment. I liked where Firode was headed with this film, he had a genuinely diagramed story, but the final execution just blew this film to shreds. Firode could have saved this film if he would have strengthened his characters, while lightening up his premise and story. I think my overall mood of this film would have changed if just these two simple directions were taken. Oh, how I only wish I could time travel back to the production of this film to show Firode the errors of his ways.  Overall, for the first time (and probably last), this was a Tautou film that I must say utterly disappointed me. From the choppy opening to the apathetic ending, I just felt that Happenstance failed due to Firode\\'s leadership and horrid marketing. Marketing is something that I didn\\'t mention before, but why would anyone purchase this film thinking that it was an Amelie 2 (per the title released in Hong Kong), and why would you place Tautou squarely on the cover knowing full well that she wasn\\'t carrying this film at all. I believe that from the first minute that passed on my DVD player, this film was in shambles. While I will applaud his subject, everything else was well below the level of mediocrity. I cannot suggest this film to anyone.  Grade: * out of *****'\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "train_data = text_dataset_from_directory(\"./movie-reviews-dataset/train\")\n",
    "test_data = text_dataset_from_directory(\"./movie-reviews-dataset/test\")\n",
    "\n",
    "\n",
    "# Prepare data remove <br> tags\n",
    "\n",
    "def prepareData(dir):\n",
    "    data = text_dataset_from_directory(dir)\n",
    "    return data.map(\n",
    "        lambda text, label: (regex_replace(text, '<br />', ' '), label)\n",
    "    )\n",
    "\n",
    "train_data = prepareData(\"./movie-reviews-dataset/train\")\n",
    "test_data = prepareData(\"./movie-reviews-dataset/test\")\n",
    "\n",
    "for text_batch, label_bath in  train_data.take(1):\n",
    "    print(text_batch.numpy()[0])\n",
    "    print(label_bath.numpy()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(1,), dtype=\"string\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Vectorization\n",
    "\n",
    "Our first layer will be `TV` which will convert of string input to a sequence of integers, each one representing a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 1000\n",
    "max_len = 100\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "     # Max vocab size. Any words outside of the max_tokens most common ones\n",
    "     # will be treated the same way: as \"out of vocabulary\" (OOV) tokens.\n",
    "     max_tokens = max_tokens,\n",
    "     output_mode=\"int\",\n",
    "     output_sequence_length=max_len\n",
    ")\n",
    "\n",
    "train_texts = train_data.map(lambda text, label: text)\n",
    "\n",
    "vectorize_layer.adapt(train_texts)\n",
    "\n",
    "model.add(vectorize_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "Our next layer will be the Embedding layer, which will convert / turn the integers produced by the previous layers into fixed-length vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we're using max_tokens + 1 here, since there's an\n",
    "# out-of-vocabulary (OOV) token that gets added to the vocab.\n",
    "model.add(Embedding(max_tokens + 1, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Recurrent Layer\n",
    "\n",
    " 64 is the \"units\" parameter, which is the\n",
    " dimensionality of the output space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping Up\n",
    "\n",
    "To finish off our network, we’ll add a standard fully-connected (Dense) layer and an output layer with sigmoid activation:\n",
    "\n",
    "The sigmoid activation outputs a number between 0 and 1, which is perfect for our problem - 0 represents a negative review, and 1 represents a positive one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "\n",
    "Before we can begin training, we need to configure the training process. We decide a few key factors during the compilation step, including:\n",
    "\n",
    " * `The optimizer`: We’ll stick with a pretty good default: the `Adam gradient-based optimizer`. Keras has many other optimizers you can look into as well. Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. How you should change your weights or learning rates of your neural network to reduce the losses is defined by the optimizers you use.\n",
    "\n",
    " * `The loss function`: Since we only have 2 output classes (positive and negative), we’ll use the `Binary Cross-Entropy loss`. A loss function measures how different the predicted output is versus the expected output. For binary classification problems, we use binary cross entropy as loss function. `Epochs` is the number of times the whole training data is used to train the model.\n",
    "\n",
    " * `A list of metrics`: Since this is a classification problem, we’ll just have Keras report on the accuracy metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 87s 106ms/step - loss: 0.5270 - accuracy: 0.7337\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 60s 77ms/step - loss: 0.4369 - accuracy: 0.7988\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.4069 - accuracy: 0.8167\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 75s 96ms/step - loss: 0.3892 - accuracy: 0.8244\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 61s 78ms/step - loss: 0.3728 - accuracy: 0.8345\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 62s 80ms/step - loss: 0.3556 - accuracy: 0.8413\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 63s 80ms/step - loss: 0.3400 - accuracy: 0.8503\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 66s 84ms/step - loss: 0.3221 - accuracy: 0.8588\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 63s 80ms/step - loss: 0.3054 - accuracy: 0.8665\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.3112 - accuracy: 0.8603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1352063b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('cnn_h5_tf', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"cnn_h5_tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "[[0.9807688]]\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "[[0.00792643]]\n"
     ]
    }
   ],
   "source": [
    "# Should print a very high score like 0.98.\n",
    "print(model.predict([\n",
    "  \"i loved it! highly recommend it to anyone and everyone looking for a great movie to watch.\",\n",
    "]))\n",
    "\n",
    "# Should print a very low score like 0.01.\n",
    "print(model.predict([\n",
    "  \"this was awful! i hated it so much, nobody should watch this. the acting was terrible, the music was terrible, overall it was just bad.\",\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving our Network\n",
    "\n",
    " * **Network Depth**\n",
    "   What happens if we add Recurrent layers? How does that affect training and/or the model’s final performance?\n",
    "   ```\n",
    "    model = Sequential()\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the full sequence instead of just the last\n",
    "    # output of the sequence.\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "\n",
    "    # This second recurrent layer's input sequence is the\n",
    "    # output sequence of the previous layer.\n",
    "    model.add(LSTM(64))\n",
    "\n",
    "   ```\n",
    " * **Dropout**\n",
    "   What if we incorporated dropout (e.g. via Dropout layers), which is commonly used to prevent overfitting?\n",
    "   ```\n",
    "   from tensorflow.keras.layers import Dropout\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Examples of common ways to use dropout below. These\n",
    "    # parameters are not necessarily the most optimal.\n",
    "    model.add(LSTM(64, dropout=0.25, recurrent_dropout=0.25))\n",
    "\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "   ```\n",
    " * **Adjusting TV parameters**\n",
    "\n",
    " * **Pre-processing**\n",
    "   All we did to clean our dataset was remove <br /> markers. There may be other pre-processing steps that would be useful to us. For example:\n",
    "\n",
    "    *  Removing “useless” tokens (e.g. ones that are extremely common or otherwise not useful)\n",
    "    \n",
    "    *  Fixing common mispellings / abbreviations and standardizing slang"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
